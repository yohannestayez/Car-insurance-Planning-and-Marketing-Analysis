{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset (replace 'data.csv' with your actual data file)\n",
    "df = pd.read_csv('C:\\\\Users\\\\Toshiba\\\\Documents\\\\Kifiya\\\\week 3\\\\Data\\\\converted_data.csv')\n",
    "\n",
    "# ======================================\n",
    "# Step 1: Data Preparation\n",
    "# ======================================\n",
    "\n",
    "# Handling missing data: Impute missing values with median for numerical and mode for categorical features\n",
    "# Apply median imputation only to numeric columns\n",
    "df.fillna(df.select_dtypes(include='number').median(), inplace=True)\n",
    "\n",
    "# For non-numeric columns (e.g., categorical, datetime), you can impute using mode (most frequent value)\n",
    "df.fillna(df.select_dtypes(exclude='number').mode().iloc[0], inplace=True)\n",
    "\n",
    "# Ensure the column is in datetime format (in case it's not)\n",
    "df['TransactionMonth'] = pd.to_datetime(df['TransactionMonth'])\n",
    "\n",
    "# Extract year, month, and day from TransactionMonth\n",
    "df['TransactionYear'] = df['TransactionMonth'].dt.year\n",
    "df['TransactionMonthOnly'] = df['TransactionMonth'].dt.month\n",
    "df['TransactionDay'] = df['TransactionMonth'].dt.day\n",
    "\n",
    "df = df.drop(columns=['VehicleIntroDate'])\n",
    "\n",
    "\n",
    "\n",
    "# Feature Engineering: Create a new feature (e.g., ClaimsRatio: TotalClaims / TotalPremium)\n",
    "df['ClaimsRatio'] = df['TotalClaims'] / df['TotalPremium']\n",
    "\n",
    "\n",
    "# Limit the number of unique categories for high-cardinality features like PostalCode\n",
    "top_n = 100  # Set a threshold for how many top categories you want to keep\n",
    "df['PostalCode'] = df['PostalCode'].apply(lambda x: x if x in df['PostalCode'].value_counts().index[:top_n] else 'Other')\n",
    "# Encoding categorical data: One-Hot Encoding for categorical variables like Province, Gender, PostalCode\n",
    "df_encoded = pd.get_dummies(df, columns=['Province', 'Gender', 'PostalCode'])\n",
    "\n",
    "# Splitting features and labels\n",
    "X = df_encoded.drop(columns=['TotalPremium', 'TotalClaims'])  # Features\n",
    "y = df_encoded['TotalPremium']  # Target variable: TotalPremium (you can also change to TotalClaims)\n",
    "\n",
    "# Train-Test Split (80% training and 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "# Step 2: Model Building\n",
    "# ======================================\n",
    "\n",
    "# Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Regressor Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost Regressor Model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ======================================\n",
    "# Step 3: Model Evaluation\n",
    "# ======================================\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"R-squared (R2): {r2:.4f}\\n\")\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "evaluate_model(lr_model, X_test, y_test, \"Linear Regression\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Evaluate XGBoost\n",
    "evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# ======================================\n",
    "# Step 4: Feature Importance Analysis\n",
    "# ======================================\n",
    "\n",
    "# Feature Importance for Random Forest\n",
    "rf_importances = rf_model.feature_importances_\n",
    "rf_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf_importances})\n",
    "rf_importance_df = rf_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nRandom Forest Feature Importances:\\n\", rf_importance_df)\n",
    "\n",
    "# Feature Importance for XGBoost\n",
    "xgb_importances = xgb_model.feature_importances_\n",
    "xgb_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_importances})\n",
    "xgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nXGBoost Feature Importances:\\n\", xgb_importance_df)\n",
    "\n",
    "# Plotting Feature Importance for XGBoost\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "# Step 5: Model Interpretability using SHAP\n",
    "# ======================================``\n",
    "\n",
    "# Initialize SHAP for XGBoost\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names=X_test.columns)\n",
    "\n",
    "# SHAP Dependence Plot for a specific feature (example: 'ClaimsRatio')\n",
    "shap.dependence_plot('ClaimsRatio', shap_values, X_test, feature_names=X_test.columns)\n",
    "\n",
    "# SHAP Force Plot for a single instance\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], feature_names=X_test.columns)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(venv4)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
